{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('data/train.csv', index_col='Id')\n",
    "test_df = pd.read_csv('data/test.csv', index_col='Id')\n",
    "\n",
    "print(f\"train data shape: {train_df.shape} \\ntest data shape: {test_df.shape}\")\n",
    "\n",
    "\n",
    "print(f\"Check null y: {train_df.SalePrice.isnull().sum()}\")\n",
    "print (f\"Check NA y: {train_df.SalePrice.isna().sum()}\")\n",
    "#print duplicates\n",
    "print(f\"Check duplicates: {train_df.duplicated().sum()}\")\n",
    "\n",
    "\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a. drop feautres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df\n",
    "\n",
    "test = test_df\n",
    "\n",
    "def drop_feautres(df):\n",
    "    df.drop(columns=[\"PoolArea\", \"MiscVal\", '3SsnPorch', 'LowQualFinSF', 'BsmtFinSF2', 'EnclosedPorch'], inplace=True)\n",
    "\n",
    "drop_feautres(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Divide into X_train and Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this is category not number \n",
    "\n",
    "def changeType(df, columnName):\n",
    "    df[columnName] = df[columnName].astype(str)\n",
    "\n",
    "\n",
    "changeType(train, 'MSSubClass')\n",
    "\n",
    "# divide columns into numerical and categories\n",
    "\n",
    "\n",
    "def set_column_types(df, set_type='train'):\n",
    "    if set_type == 'train':\n",
    "        num_cols = df.drop(columns='SalePrice').select_dtypes(include='number').columns\n",
    "    else:\n",
    "        num_cols = df.select_dtypes(include='number').columns\n",
    "\n",
    "    cat_cols = df.select_dtypes(include='object').columns\n",
    "    return num_cols, cat_cols\n",
    "\n",
    "num_cols, cat_cols = set_column_types(train)\n",
    "\n",
    "\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Verify & handle Numerical Missing Values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def handle_missing_numerical(df):\n",
    "    print(f\"Missing numerical values:\\n{df[num_cols].isna().sum().sort_values(ascending=False).head()}\")\n",
    "\n",
    "    # GarageYrBlt       81\n",
    "    # MasVnrArea         8\n",
    "\n",
    "    #### Lot Frontage\n",
    "    df.loc[df.LotFrontage.isna(), ['Street', 'LotFrontage']]\n",
    "    # LotFrontage      259 → for Na sreet is there so mean value will be applide\n",
    "    df.LotFrontage.fillna(df.LotFrontage.mean(), inplace=True)\n",
    "    #### Garage \n",
    "    # Garage type Nan means no garage so year should 0\n",
    "    df.loc[df.GarageYrBlt.isna(), df.columns.str.contains('Garage', case=False)]\n",
    "    df.GarageYrBlt.fillna(0, inplace=True)\n",
    "    # Masonry\n",
    "    # masnory type is none so area should be 0\n",
    "    df.loc[df.MasVnrArea.isna(), df.columns.str.contains('MasVnr', case=False)]\n",
    "    df.MasVnrArea.fillna(0, inplace=True)\n",
    "    print(f\"\\nAfter processing:\\n{df[num_cols].isna().sum().sort_values(ascending=False).head()}\")\n",
    "\n",
    "handle_missing_numerical(train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. handle outliers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Verify features aftecting price \n",
    "corr = pd.DataFrame({'SalePrice': train_df.drop(columns=cat_cols).corr()['SalePrice'].sort_values(ascending=False)}).iloc[1:]\n",
    "corr.style.background_gradient(cmap='YlOrRd')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on graph for similicty for outlier i'm picking top features: \n",
    "cols_to_verify = ['GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF', '1stFlrSF', 'LotArea' ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 80))\n",
    "\n",
    "for count, feature in enumerate(cols_to_verify):\n",
    "    plt.subplot(12, 3, count+1)\n",
    "    plt.scatter(train[cols_to_verify].loc[:, feature], train.SalePrice)\n",
    "    plt.xlabel(feature, fontsize=16)\n",
    "    plt.ylabel('Sale Price', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_outliers(df):\n",
    "    \"\"\"\n",
    "    Identify outliers based on specific conditions for each feature\n",
    "    Returns the indices of outlier rows\n",
    "    \"\"\"\n",
    "    outlier_indices = []\n",
    "    \n",
    "   # GrLivArea outliers (large area, low price)\n",
    "    condition1 = (df['GrLivArea'] > 4000) & (df.SalePrice < 300000)\n",
    "    indices1 = df[condition1].index.tolist()\n",
    "    outlier_indices.extend(indices1)\n",
    "    print(f\"Found {len(indices1)} outliers with large GrLivArea but low price\")\n",
    "    \n",
    "    # GrLivArea high-end outliers (expensive houses)\n",
    "    condition2 = (df['GrLivArea'] > 4000) & (df.SalePrice > 650000)\n",
    "    indices2 = df[condition2].index.tolist()\n",
    "    outlier_indices.extend(indices2)\n",
    "    print(f\"Found {len(indices2)} outliers with large GrLivArea and very high price\")\n",
    "    \n",
    "    # TotalBsmtSF outliers (large basement, low price)\n",
    "    condition3 = (df['TotalBsmtSF'] > 3000) & (df.SalePrice < 250000)\n",
    "    indices3 = df[condition3].index.tolist()\n",
    "    outlier_indices.extend(indices3)\n",
    "    print(f\"Found {len(indices3)} outliers with large TotalBsmtSF but low price\")\n",
    "    \n",
    "    # 1stFlrSF outliers (large first floor, unusual price)\n",
    "    condition4 = (df['1stFlrSF'] > 3000)   & (df.SalePrice < 300000)\n",
    "    indices4 = df[condition4].index.tolist()\n",
    "    outlier_indices.extend(indices4)\n",
    "    print(f\"Found {len(indices4)} outliers with unusually large 1stFlrSF\")\n",
    "    \n",
    "    # GarageArea outliers (large garage, low price)w\n",
    "    condition5 = (df['GarageArea'] > 1200) & (df.SalePrice < 200000)\n",
    "    indices5 = df[condition5].index.tolist()\n",
    "    outlier_indices.extend(indices5)\n",
    "    print(f\"Found {len(indices5)} outliers with large GarageArea but low price\")\n",
    "    \n",
    "    # GarageCars outliers (4 car garages)\n",
    "    condition6 = (df['GarageCars'] == 4)\n",
    "    indices6 = df[condition6].index.tolist()\n",
    "    outlier_indices.extend(indices6)\n",
    "    print(f\"Found {len(indices6)} outliers with 4 car garages\")\n",
    "    \n",
    "    # Very high-priced houses (general luxury outliers)\n",
    "    condition7 = (df.SalePrice > 650000)\n",
    "    indices7 = df[condition7].index.tolist()\n",
    "    outlier_indices.extend(indices7)\n",
    "    print(f\"Found {len(indices7)} outliers with very high prices (>$650,000)\")\n",
    "\n",
    "    # Lot Area\n",
    "    indices9 = df[df.LotArea > 100000].index.tolist()\n",
    "    outlier_indices.extend(indices9)\n",
    "    print(f\"Found {len(indices9)} outliers with very large lot area > 100k\")\n",
    "\n",
    "\n",
    "    # Remove duplicates (a row might satisfy multiple conditions)\n",
    "    outlier_indices = list(set(outlier_indices))\n",
    "    print(f\"Total unique outliers found: {len(outlier_indices)}\")\n",
    "    \n",
    "    return outlier_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "outlier_indices = identify_outliers(train)\n",
    "\n",
    "\n",
    "train = train.drop(outlier_indices)\n",
    "# have to remove both to mach shape \n",
    "print(f\"Original shape: {train_df.shape}, Clean shape: {train.shape}\")\n",
    "\n",
    "print(f\"\\nAfter processing:\\n{train[num_cols].isna().sum().sort_values(ascending=False).head()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Categorical Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fill_missing_categoricals(df):\n",
    "    print(df[cat_cols].isna().sum().sort_values(ascending=False).head(5))\n",
    "\n",
    "\n",
    "    # just fill with 'Na' \n",
    "    df[cat_cols] = df[cat_cols].fillna('Na')\n",
    "    print(f\"\\n\\nAfter processing: \\n{df[cat_cols].isna().sum().sort_values(ascending=False).head(5)}\")\n",
    "    \n",
    "fill_missing_categoricals(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features_cols = ['Alley', 'LotShape', 'Utilities', 'LandSlope', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC',\n",
    "        'Functional', 'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'PavedDrive', 'KitchenQual']\n",
    "\n",
    "ordinal_mappings = {\n",
    "        # Alley access\n",
    "        'Alley': {np.nan: 0 , 'NA': 0, 'Grvl': 1, 'Pave': 2},\n",
    "        \n",
    "        # Lot shape\n",
    "        'LotShape': {'IR3': 0, 'IR2': 1, 'IR1': 2, 'Reg': 3},\n",
    "        \n",
    "        # Utilities\n",
    "        'Utilities': {'NoSeWa': 0, 'NoSewr': 1, 'AllPub': 3},\n",
    "        \n",
    "        # Land Slope\n",
    "        'LandSlope': {'Sev': 0, 'Mod': 1, 'Gtl': 2},\n",
    "        \n",
    "        # Quality ratings (consistent pattern across several features)\n",
    "        'ExterQual': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "        'ExterCond': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "        'BsmtQual': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "        'BsmtCond': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "        'HeatingQC': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "        'FireplaceQu': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "        'GarageQual': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "        'GarageCond': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "        'PoolQC': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n",
    "        \n",
    "        # Basement exposure\n",
    "        'BsmtExposure': {'NA': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n",
    "        \n",
    "        # Basement finished types\n",
    "        'BsmtFinType1': {'NA': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n",
    "        'BsmtFinType2': {'NA': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n",
    "        \n",
    "        # Home functionality\n",
    "        'Functional': {'Sal': 1, 'Sev': 2, 'Maj2': 3, 'Maj1': 4, 'Mod': 5, 'Min2': 6, 'Min1': 7, 'Typ': 8},\n",
    "        \n",
    "        # Garage finish\n",
    "        'GarageFinish': {'NA': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3},\n",
    "        \n",
    "        # Fence quality\n",
    "        'Fence': {'NA': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4},\n",
    "\n",
    "        #Kitchen quality\n",
    "        'KitchenQual' : {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd' : 4, 'Ex' : 5},\n",
    "\n",
    "        # paved driveway \n",
    "        'PavedDrive' : {'N': 1, 'P': 2, 'Y': 3}\n",
    "    }\n",
    "\n",
    "ordinal_features = []\n",
    "binary_features = []\n",
    "multi_features = []\n",
    "\n",
    "\n",
    "def categorise_features(train):\n",
    "    if len(ordinal_features) > 0 and len(binary_features) > 0 and len(multi_features) > 0:\n",
    "        return\n",
    "    \n",
    "    \"\"\"\n",
    "    Categorise features into 3 groups for further processing\n",
    "    \"\"\"\n",
    "    \n",
    "    cat_info = {}\n",
    "\n",
    "\n",
    "    for col in cat_cols:    \n",
    "        unique_vals =  train[col].value_counts(dropna=False).shape[0]\n",
    "        missing_vals = train[col].isna().sum()\n",
    "        sample_vals = train[col].value_counts().head(3).index.tolist()\n",
    "        \n",
    "        cat_info[col] = {\n",
    "            'unique_values': unique_vals,\n",
    "            'missing_values': missing_vals,\n",
    "            'examples': sample_vals\n",
    "        }\n",
    "\n",
    "    for col, info in cat_info.items():\n",
    "        if col in ordinal_features_cols:\n",
    "            ordinal_features.append(col)\n",
    "        elif info['unique_values'] == 2:\n",
    "            binary_features.append(col)\n",
    "        else:\n",
    "            multi_features.append(col)\n",
    "     \n",
    "\n",
    "def process_binary_features(train):\n",
    "        \"\"\"\n",
    "        Modify binary descriptive features to be 0 or 1 \n",
    "        \"\"\"\n",
    "        binary_mappings = {\n",
    "            'Street': {'Pave': 1, 'Grvl': 0},\n",
    "            'CentralAir': {'Y': 1, 'N': 0}\n",
    "        }\n",
    "        for feature, mapping in binary_mappings.items():\n",
    "            train[feature] = train[feature].map(mapping)\n",
    "    \n",
    "    # Ensure any unexpected values (if they appear) are handled\n",
    "        if train[feature].isna().any():\n",
    "            # print(f\"Warning: Unexpected values in {feature} - filling with mode\")\n",
    "            train[feature] = train[feature].fillna(train[feature].mode()[0])\n",
    "\n",
    "        # for feature in binary_features:\n",
    "        #     print(f\"{feature} after encoding: {train[feature].value_counts().to_dict()}\") \n",
    "\n",
    "def process_ordinal_features(train):\n",
    "    \"\"\"\n",
    "    Modify ordinal descriptive features to numerical values\n",
    "    \"\"\"\n",
    "    # for each entry in ordinal_features, apply the mapping\n",
    "    for feature in ordinal_features:\n",
    "        # print(f\"DEBUG: current feature: {feature}\")\n",
    "        train[feature] = train[feature].map(ordinal_mappings[feature])\n",
    "        # print(f\"{feature} after encoding: {train[feature].value_counts().to_dict()}\")\n",
    "        # Ensure any unexpected values (if they appear) are handled\n",
    "        if train[feature].isna().any():\n",
    "            # print(f\"Warning: Unexpected values in {feature} - filling with mode\")\n",
    "            train[feature] = train[feature].fillna(train[feature].mode()[0])\n",
    "        # print(f\"{feature} after encoding: {train[feature].value_counts().to_dict()}\")\n",
    "\n",
    "def process_multi_features(train):\n",
    "    encoded_features = []\n",
    "    for feature in multi_features:\n",
    "        multi = pd.get_dummies(train[feature], prefix=f'{feature[0:4] + feature[-1]}', prefix_sep='_')\n",
    "        encoded_features.append(multi)\n",
    "    encoded_df = pd.concat(encoded_features, axis=1)\n",
    "    train = train.drop(columns=multi_features)\n",
    "    train = pd.concat([train, encoded_df], axis=1)\n",
    "    return train\n",
    "     \n",
    "\n",
    "\n",
    "def process_all_cateorical_features(df):\n",
    "    categorise_features(df)\n",
    "    print (f\"\\nBinary features: {binary_features}\\n\")\n",
    "    print (f\"Ordinal features: {ordinal_features}\\n\")\n",
    "    print (f\"Multi features: {multi_features}\\n\")\n",
    "    process_binary_features(df)\n",
    "    process_ordinal_features(df)\n",
    "    df = process_multi_features(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "train = process_all_cateorical_features(train)\n",
    "\n",
    "train.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transform output \n",
    "- only y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# transform the price : \n",
    "# present it : \n",
    "def plot_dist(column, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(column, kde=True)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Price\")\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "plot_dist(train.SalePrice, \"Before log mod: Distribution of Sale Price\") \n",
    "train.SalePrice = np.log(train.SalePrice)\n",
    "plot_dist(train.SalePrice ,\"After log mod: Distribution of Sale Price\")\n",
    "\n",
    "train.SalePrice.to_csv('data/y_train.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scale Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.B Apply Scaling only on  numerical_continues_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "def scale_selected_numerical_features(df) -> pd.DataFrame:\n",
    "    \"\"\"Drops columns which are not continues and scales rest numerical\n",
    "\n",
    "    Args:\n",
    "        df (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    if 'SalePrice' in df.columns:\n",
    "        df.drop(columns=['SalePrice'], inplace=True)\n",
    "\n",
    "    \n",
    "    numerical_discrete_cols = ['OverallQual', 'OverallCond', 'BsmtFullBath', 'BsmtHalfBath' , 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', \n",
    "                 'Fireplaces', 'GarageCars']\n",
    "    \n",
    "    numerical_df = df[num_cols]\n",
    "    numerical_continues_cols = numerical_df.drop(columns=numerical_discrete_cols).columns\n",
    "\n",
    "    # create separate df for scaling \n",
    "    numerical_continues_df = df[numerical_continues_cols]\n",
    "\n",
    "    # drop orginal columns → those will be scaled and re-added \n",
    "    df.drop(columns=numerical_continues_cols, inplace=True)\n",
    "    # print(f\"No of columns after drop: {len(df.columns)}\")\n",
    "\n",
    "    # process scaling \n",
    "    scaler = StandardScaler()\n",
    "    numerical_continues_scalled_array = scaler.fit_transform(numerical_continues_df)\n",
    "    numerical_continues_scalled_df = pd.DataFrame(numerical_continues_scalled_array, columns=numerical_continues_df.columns, index=numerical_continues_df.index)\n",
    "\n",
    "    # now need to apply log transformaiton for few features \n",
    "    # Features with very high scaling values that need transformation\n",
    "    log_transform_features = [\n",
    "        'LotFrontage',  'LotArea', \n",
    "        'MasVnrArea', 'ScreenPorch', 'OpenPorchSF'\n",
    "    ]  \n",
    "\n",
    "    # print(f\"Is any null before log transform: {numerical_continues_scalled_df.isna().sum().sort_values(ascending=False)}\")\n",
    "\n",
    "\n",
    "#####  DEBUG\n",
    "    for feature in ['LotFrontage', 'LotArea']:\n",
    "        # Print min and max values\n",
    "        print(f\"{feature} min: {numerical_continues_scalled_df[feature].min()}\")\n",
    "        print(f\"{feature} max: {numerical_continues_scalled_df[feature].max()}\")\n",
    "        \n",
    "        # Check if any values are close to -1\n",
    "        near_negative_one = sum((numerical_continues_scalled_df[feature] < -0.99) & \n",
    "                                (numerical_continues_scalled_df[feature] > -1.01))\n",
    "        print(f\"{feature} values near -1: {near_negative_one}\")\n",
    "        \n",
    "        # Check for infinite values\n",
    "        inf_values = sum(np.isinf(numerical_continues_scalled_df[feature]))\n",
    "        print(f\"{feature} infinite values: {inf_values}\")\n",
    "        \n",
    "        # Look at a few examples of values that might be causing problems\n",
    "        problematic = numerical_continues_scalled_df[numerical_continues_scalled_df[feature] < -0.9]\n",
    "        if not problematic.empty:\n",
    "            print(f\"Sample problematic values for {feature}:\")\n",
    "            print(problematic[feature].head())\n",
    "\n",
    "\n",
    "\n",
    "#########\n",
    "\n",
    "    for feature in log_transform_features:\n",
    "        numerical_continues_scalled_df[f'{feature}_log'] = np.log1p(numerical_continues_scalled_df[feature])\n",
    "        numerical_continues_scalled_df.drop(columns=[feature], inplace=True)\n",
    "\n",
    "    # print(f\"Mean values of scaled df\\n\\n{numerical_continues_scalled_df.mean().sort_values(ascending=False)}\")\n",
    "    # print(f\"MAX values of scaled df\\n\\n{numerical_continues_scalled_df.max().sort_values(ascending=False)}\")\n",
    "    \n",
    "\n",
    "\n",
    "    ##  very unclean workaround → fill na calused by log transform with 0 \n",
    "    \n",
    "    numerical_continues_scalled_df.fillna(0, inplace=True)\n",
    "\n",
    "    return numerical_continues_scalled_df\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"No of columns = {len(train.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train df \n",
    "\n",
    "new_train_partital_df = scale_selected_numerical_features(train)\n",
    "# print(f\"Is any null in new partial log transform: {new_train_partital_df.isna().sum().sum()}\")\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"Partial columns lenght={len(new_train_partital_df.columns)}\")\n",
    "\n",
    "X_train = pd.merge(train, new_train_partital_df, left_index=True, right_index=True)\n",
    "\n",
    "# # save the data to csv\n",
    "# X_train = X_train.reset_index(names=\"Id\")\n",
    "# X_train.isna().sum().sort_values(ascending=False)\n",
    "# print(f\"No of columns = {len(X_train.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST DF \n",
    "test = pd.read_csv('data/test.csv', index_col='Id')\n",
    "\n",
    "#1 changeType\n",
    "changeType(test, 'MSSubClass')\n",
    "\n",
    "#2 drop_feautres\n",
    "drop_feautres(test)\n",
    "\n",
    "#3 set_column_types\n",
    "num_cols, cat_cols = set_column_types(test, \"test\")\n",
    "\n",
    "#4 handle_missing_numerical(test)\n",
    "handle_missing_numerical(test)\n",
    "\n",
    "#5 fill_missing_categoricals\n",
    "fill_missing_categoricals(test)\n",
    "\n",
    "test = process_all_cateorical_features(test)\n",
    "\n",
    "\n",
    "\n",
    "new_test_partital_df = scale_selected_numerical_features(test)\n",
    "print(f\"Partial columns lenght={len(new_train_partital_df.columns)}\")\n",
    "\n",
    "X_test = pd.merge(test, new_test_partital_df, left_index=True, right_index=True)\n",
    "\n",
    "X_test.head()\n",
    "# print(f\"No of columns = {len(X_train.columns)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c merge dfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIND DIFFS IN COLUMNS BETWEEN X_TRAIN and X_TEST\n",
    "set(X_train.columns.to_list()).symmetric_difference(set(X_test.columns.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "exted_columns = X_test.filter(like='Exted').columns.to_list()\n",
    "extet_replacements = []\n",
    "\n",
    "for col in exted_columns:\n",
    "    # Create corresponding 'Extet' name from 'Exted'\n",
    "    new_name = col.replace('Exted', 'Extet')\n",
    "    extet_replacements.append(new_name)\n",
    "\n",
    "rename_dict = dict(zip(exted_columns, extet_replacements))\n",
    "X_test.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Example: Fix Electrical column inconsistencies\n",
    "elecl_columns = X_test.filter(like='Elecl').columns.to_list()\n",
    "if elecl_columns:\n",
    "    # Map to corresponding columns in train dataset\n",
    "    elecl_replacements = [col.replace('Elecl', 'Electrical') for col in elecl_columns]\n",
    "    rename_dict = dict(zip(elecl_columns, elecl_replacements))\n",
    "    X_test.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "\n",
    "X_train, X_test = X_train.align(X_test, join='outer', axis=1, fill_value=0)\n",
    "X_test.fillna(0, inplace=True)\n",
    "\n",
    "X_train.to_csv('data/X_train_cleaned.csv', index=True, index_label='Id')\n",
    "X_test.to_csv('data/X_test_cleaned.csv', index=True, index_label='Id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(X_train.columns.to_list()).symmetric_difference(set(X_test.columns.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.isna().sum().sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ml-intro_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
