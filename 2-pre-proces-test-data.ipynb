{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. remove those columns with missing data in > 50% cases \n",
    "- only x_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (test_df.isnull().mean() * 100).sort_values(ascending=False)\n",
    "# print(result)\n",
    "columns_to_drop = result.loc[result > 47].index # 47% as threshold for this data set\n",
    "column_to_drop_list = columns_to_drop.tolist()\n",
    "print(column_to_drop_list)\n",
    "test_df = test_df.drop(columns=column_to_drop_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Handle Remaining Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# step1 extract columns with missing values and divide into numerical and categorical columns\n",
    "\n",
    "result =  ((test_df.isnull().sum()).sort_values(ascending=False)) \n",
    "columns_with_missing_values = result.loc[result > 0]\n",
    "# instead of above - better since i'm looking for columns with missing values\n",
    "columns_with_missing_values = test_df.columns[test_df.isnull().any()]\n",
    "# print(f\"Colums with missing values {columns_with_missing_values}, type {type(columns_with_missing_values)}\")\n",
    "null_cols_df= test_df[columns_with_missing_values]\n",
    "# print(f\"Only null columns df: {null_cols_df}\")\n",
    "\n",
    "null_cols_types = null_cols_df.dtypes\n",
    "# print(f\"Null columns types: \\n{null_cols_types}\")\n",
    "\n",
    "\n",
    "num_cols = null_cols_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "cat_cols = null_cols_df.select_dtypes(include=['object']).columns\n",
    "print(f\"cat num_cols: {num_cols}\")\n",
    "\n",
    "# step 2: create imputers for numerical and categorical columns\n",
    "\n",
    "num_imputer = SimpleImputer(strategy='mean') # for numerical columns \n",
    "cat_imputer = SimpleImputer(strategy='most_frequent') # for categorical columns \n",
    "\n",
    "# step 3 apply imputers to columns with missing values\n",
    "\n",
    "test_df[num_cols] = num_imputer.fit_transform(test_df[num_cols])\n",
    "test_df[cat_cols] = cat_imputer.fit_transform(test_df[cat_cols])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. convert categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_cols = test_df.select_dtypes(include=['object']).columns\n",
    "cat_cols\n",
    "\n",
    "# this is simple aproach to be improved later \n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    test_df[col] = le.fit_transform(test_df[col])\n",
    "\n",
    "test_df.info()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Feature scaling \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# train_df.drop(columns=['SalePrice'], inplace=True)\n",
    "\n",
    "\n",
    "test_ids = test_df['Id'].copy()\n",
    "test_df_no_id = test_df.drop('Id', axis=1)\n",
    "\n",
    "# print(train_df.head())\n",
    "test_df_no_id_scaled_array = scaler.fit_transform(test_df_no_id)\n",
    "test_df_no_id_scaled = pd.DataFrame(test_df_no_id_scaled_array, columns=test_df_no_id.columns)\n",
    "\n",
    "test_df_no_id_scaled['Id'] = test_ids.values\n",
    "\n",
    "test_df = test_df_no_id_scaled.copy() \n",
    "\n",
    "test_df.head()\n",
    "\n",
    "# save the data to csv\n",
    "test_df.to_csv('data/X_test_cleaned.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-intro_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
